{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WPTrqGKQob3K"
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "dataset = []\n",
    "files = open('finefoods.txt',encoding = 'ISO-8859-1')\n",
    "for one_instance in files.readlines():\n",
    "  temp = one_instance.split()\n",
    "  dataset.append(temp)\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f23zYwc2shjU"
   },
   "outputs": [],
   "source": [
    "# get the reviews rows\n",
    "reviews = []\n",
    "for i in range(len(dataset)):\n",
    "  if len(dataset[i]) == 0:\n",
    "    continue\n",
    "  if dataset[i][0] == 'review/text:': # use key word to get reviews\n",
    "    reviews.append(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fidjwvh0xWM0",
    "outputId": "cf91eeb1-28f5-43fd-87ca-64970e60f4fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568454"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews) # the length of reviews is correct. Equals to original number of set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GnU8AOr9yGci"
   },
   "outputs": [],
   "source": [
    "unique_word = [j for sub in reviews for j in sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0L3laL3Yy09W"
   },
   "outputs": [],
   "source": [
    "L = set(unique_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Y22Yi1mVzf1b"
   },
   "outputs": [],
   "source": [
    "# remove the title of 'review/test:'\n",
    "L.remove('review/text:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2OThYfZrqhfV"
   },
   "outputs": [],
   "source": [
    "# get the long stop word list from https://www.ranks.nl/stopwords \n",
    "long_stop_word_list = [\"\"\"a\n",
    "able\n",
    "about\n",
    "above\n",
    "abst\n",
    "accordance\n",
    "according\n",
    "accordingly\n",
    "across\n",
    "act\n",
    "actually\n",
    "added\n",
    "adj\n",
    "affected\n",
    "affecting\n",
    "affects\n",
    "after\n",
    "afterwards\n",
    "again\n",
    "against\n",
    "ah\n",
    "all\n",
    "almost\n",
    "alone\n",
    "along\n",
    "already\n",
    "also\n",
    "although\n",
    "always\n",
    "am\n",
    "among\n",
    "amongst\n",
    "an\n",
    "and\n",
    "announce\n",
    "another\n",
    "any\n",
    "anybody\n",
    "anyhow\n",
    "anymore\n",
    "anyone\n",
    "anything\n",
    "anyway\n",
    "anyways\n",
    "anywhere\n",
    "apparently\n",
    "approximately\n",
    "are\n",
    "aren\n",
    "arent\n",
    "arise\n",
    "around\n",
    "as\n",
    "aside\n",
    "ask\n",
    "asking\n",
    "at\n",
    "auth\n",
    "available\n",
    "away\n",
    "awfully\n",
    "b\n",
    "back\n",
    "be\n",
    "became\n",
    "because\n",
    "become\n",
    "becomes\n",
    "becoming\n",
    "been\n",
    "before\n",
    "beforehand\n",
    "begin\n",
    "beginning\n",
    "beginnings\n",
    "begins\n",
    "behind\n",
    "being\n",
    "believe\n",
    "below\n",
    "beside\n",
    "besides\n",
    "between\n",
    "beyond\n",
    "biol\n",
    "both\n",
    "brief\n",
    "briefly\n",
    "but\n",
    "by\n",
    "c\n",
    "ca\n",
    "came\n",
    "can\n",
    "cannot\n",
    "can't\n",
    "cause\n",
    "causes\n",
    "certain\n",
    "certainly\n",
    "co\n",
    "com\n",
    "come\n",
    "comes\n",
    "contain\n",
    "containing\n",
    "contains\n",
    "could\n",
    "couldnt\n",
    "d\n",
    "date\n",
    "did\n",
    "didn't\n",
    "different\n",
    "do\n",
    "does\n",
    "doesn't\n",
    "doing\n",
    "done\n",
    "don't\n",
    "down\n",
    "downwards\n",
    "due\n",
    "during\n",
    "e\n",
    "each\n",
    "ed\n",
    "edu\n",
    "effect\n",
    "eg\n",
    "eight\n",
    "eighty\n",
    "either\n",
    "else\n",
    "elsewhere\n",
    "end\n",
    "ending\n",
    "enough\n",
    "especially\n",
    "et\n",
    "et-al\n",
    "etc\n",
    "even\n",
    "ever\n",
    "every\n",
    "everybody\n",
    "everyone\n",
    "everything\n",
    "everywhere\n",
    "ex\n",
    "except\n",
    "f\n",
    "far\n",
    "few\n",
    "ff\n",
    "fifth\n",
    "first\n",
    "five\n",
    "fix\n",
    "followed\n",
    "following\n",
    "follows\n",
    "for\n",
    "former\n",
    "formerly\n",
    "forth\n",
    "found\n",
    "four\n",
    "from\n",
    "further\n",
    "furthermore\n",
    "g\n",
    "gave\n",
    "get\n",
    "gets\n",
    "getting\n",
    "give\n",
    "given\n",
    "gives\n",
    "giving\n",
    "go\n",
    "goes\n",
    "gone\n",
    "got\n",
    "gotten\n",
    "h\n",
    "had\n",
    "happens\n",
    "hardly\n",
    "has\n",
    "hasn't\n",
    "have\n",
    "haven't\n",
    "having\n",
    "he\n",
    "hed\n",
    "hence\n",
    "her\n",
    "here\n",
    "hereafter\n",
    "hereby\n",
    "herein\n",
    "heres\n",
    "hereupon\n",
    "hers\n",
    "herself\n",
    "hes\n",
    "hi\n",
    "hid\n",
    "him\n",
    "himself\n",
    "his\n",
    "hither\n",
    "home\n",
    "how\n",
    "howbeit\n",
    "however\n",
    "hundred\n",
    "i\n",
    "id\n",
    "ie\n",
    "if\n",
    "i'll\n",
    "im\n",
    "immediate\n",
    "immediately\n",
    "importance\n",
    "important\n",
    "in\n",
    "inc\n",
    "indeed\n",
    "index\n",
    "information\n",
    "instead\n",
    "into\n",
    "invention\n",
    "inward\n",
    "is\n",
    "isn't\n",
    "it\n",
    "itd\n",
    "it'll\n",
    "its\n",
    "itself\n",
    "i've\n",
    "j\n",
    "just\n",
    "k\n",
    "keep\tkeeps\n",
    "kept\n",
    "kg\n",
    "km\n",
    "know\n",
    "known\n",
    "knows\n",
    "l\n",
    "largely\n",
    "last\n",
    "lately\n",
    "later\n",
    "latter\n",
    "latterly\n",
    "least\n",
    "less\n",
    "lest\n",
    "let\n",
    "lets\n",
    "like\n",
    "liked\n",
    "likely\n",
    "line\n",
    "little\n",
    "'ll\n",
    "look\n",
    "looking\n",
    "looks\n",
    "ltd\n",
    "m\n",
    "made\n",
    "mainly\n",
    "make\n",
    "makes\n",
    "many\n",
    "may\n",
    "maybe\n",
    "me\n",
    "mean\n",
    "means\n",
    "meantime\n",
    "meanwhile\n",
    "merely\n",
    "mg\n",
    "might\n",
    "million\n",
    "miss\n",
    "ml\n",
    "more\n",
    "moreover\n",
    "most\n",
    "mostly\n",
    "mr\n",
    "mrs\n",
    "much\n",
    "mug\n",
    "must\n",
    "my\n",
    "myself\n",
    "n\n",
    "na\n",
    "name\n",
    "namely\n",
    "nay\n",
    "nd\n",
    "near\n",
    "nearly\n",
    "necessarily\n",
    "necessary\n",
    "need\n",
    "needs\n",
    "neither\n",
    "never\n",
    "nevertheless\n",
    "new\n",
    "next\n",
    "nine\n",
    "ninety\n",
    "no\n",
    "nobody\n",
    "non\n",
    "none\n",
    "nonetheless\n",
    "noone\n",
    "nor\n",
    "normally\n",
    "nos\n",
    "not\n",
    "noted\n",
    "nothing\n",
    "now\n",
    "nowhere\n",
    "o\n",
    "obtain\n",
    "obtained\n",
    "obviously\n",
    "of\n",
    "off\n",
    "often\n",
    "oh\n",
    "ok\n",
    "okay\n",
    "old\n",
    "omitted\n",
    "on\n",
    "once\n",
    "one\n",
    "ones\n",
    "only\n",
    "onto\n",
    "or\n",
    "ord\n",
    "other\n",
    "others\n",
    "otherwise\n",
    "ought\n",
    "our\n",
    "ours\n",
    "ourselves\n",
    "out\n",
    "outside\n",
    "over\n",
    "overall\n",
    "owing\n",
    "own\n",
    "p\n",
    "page\n",
    "pages\n",
    "part\n",
    "particular\n",
    "particularly\n",
    "past\n",
    "per\n",
    "perhaps\n",
    "placed\n",
    "please\n",
    "plus\n",
    "poorly\n",
    "possible\n",
    "possibly\n",
    "potentially\n",
    "pp\n",
    "predominantly\n",
    "present\n",
    "previously\n",
    "primarily\n",
    "probably\n",
    "promptly\n",
    "proud\n",
    "provides\n",
    "put\n",
    "q\n",
    "que\n",
    "quickly\n",
    "quite\n",
    "qv\n",
    "r\n",
    "ran\n",
    "rather\n",
    "rd\n",
    "re\n",
    "readily\n",
    "really\n",
    "recent\n",
    "recently\n",
    "ref\n",
    "refs\n",
    "regarding\n",
    "regardless\n",
    "regards\n",
    "related\n",
    "relatively\n",
    "research\n",
    "respectively\n",
    "resulted\n",
    "resulting\n",
    "results\n",
    "right\n",
    "run\n",
    "s\n",
    "said\n",
    "same\n",
    "saw\n",
    "say\n",
    "saying\n",
    "says\n",
    "sec\n",
    "section\n",
    "see\n",
    "seeing\n",
    "seem\n",
    "seemed\n",
    "seeming\n",
    "seems\n",
    "seen\n",
    "self\n",
    "selves\n",
    "sent\n",
    "seven\n",
    "several\n",
    "shall\n",
    "she\n",
    "shed\n",
    "she'll\n",
    "shes\n",
    "should\n",
    "shouldn't\n",
    "show\n",
    "showed\n",
    "shown\n",
    "showns\n",
    "shows\n",
    "significant\n",
    "significantly\n",
    "similar\n",
    "similarly\n",
    "since\n",
    "six\n",
    "slightly\n",
    "so\n",
    "some\n",
    "somebody\n",
    "somehow\n",
    "someone\n",
    "somethan\n",
    "something\n",
    "sometime\n",
    "sometimes\n",
    "somewhat\n",
    "somewhere\n",
    "soon\n",
    "sorry\n",
    "specifically\n",
    "specified\n",
    "specify\n",
    "specifying\n",
    "still\n",
    "stop\n",
    "strongly\n",
    "sub\n",
    "substantially\n",
    "successfully\n",
    "such\n",
    "sufficiently\n",
    "suggest\n",
    "sup\n",
    "sure\tt\n",
    "take\n",
    "taken\n",
    "taking\n",
    "tell\n",
    "tends\n",
    "th\n",
    "than\n",
    "thank\n",
    "thanks\n",
    "thanx\n",
    "that\n",
    "that'll\n",
    "thats\n",
    "that've\n",
    "the\n",
    "their\n",
    "theirs\n",
    "them\n",
    "themselves\n",
    "then\n",
    "thence\n",
    "there\n",
    "thereafter\n",
    "thereby\n",
    "thered\n",
    "therefore\n",
    "therein\n",
    "there'll\n",
    "thereof\n",
    "therere\n",
    "theres\n",
    "thereto\n",
    "thereupon\n",
    "there've\n",
    "these\n",
    "they\n",
    "theyd\n",
    "they'll\n",
    "theyre\n",
    "they've\n",
    "think\n",
    "this\n",
    "those\n",
    "thou\n",
    "though\n",
    "thoughh\n",
    "thousand\n",
    "throug\n",
    "through\n",
    "throughout\n",
    "thru\n",
    "thus\n",
    "til\n",
    "tip\n",
    "to\n",
    "together\n",
    "too\n",
    "took\n",
    "toward\n",
    "towards\n",
    "tried\n",
    "tries\n",
    "truly\n",
    "try\n",
    "trying\n",
    "ts\n",
    "twice\n",
    "two\n",
    "u\n",
    "un\n",
    "under\n",
    "unfortunately\n",
    "unless\n",
    "unlike\n",
    "unlikely\n",
    "until\n",
    "unto\n",
    "up\n",
    "upon\n",
    "ups\n",
    "us\n",
    "use\n",
    "used\n",
    "useful\n",
    "usefully\n",
    "usefulness\n",
    "uses\n",
    "using\n",
    "usually\n",
    "v\n",
    "value\n",
    "various\n",
    "'ve\n",
    "very\n",
    "via\n",
    "viz\n",
    "vol\n",
    "vols\n",
    "vs\n",
    "w\n",
    "want\n",
    "wants\n",
    "was\n",
    "wasnt\n",
    "way\n",
    "we\n",
    "wed\n",
    "welcome\n",
    "we'll\n",
    "went\n",
    "were\n",
    "werent\n",
    "we've\n",
    "what\n",
    "whatever\n",
    "what'll\n",
    "whats\n",
    "when\n",
    "whence\n",
    "whenever\n",
    "where\n",
    "whereafter\n",
    "whereas\n",
    "whereby\n",
    "wherein\n",
    "wheres\n",
    "whereupon\n",
    "wherever\n",
    "whether\n",
    "which\n",
    "while\n",
    "whim\n",
    "whither\n",
    "who\n",
    "whod\n",
    "whoever\n",
    "whole\n",
    "who'll\n",
    "whom\n",
    "whomever\n",
    "whos\n",
    "whose\n",
    "why\n",
    "widely\n",
    "willing\n",
    "wish\n",
    "with\n",
    "within\n",
    "without\n",
    "wont\n",
    "words\n",
    "world\n",
    "would\n",
    "wouldnt\n",
    "www\n",
    "x\n",
    "y\n",
    "yes\n",
    "yet\n",
    "you\n",
    "youd\n",
    "you'll\n",
    "your\n",
    "youre\n",
    "yours\n",
    "yourself\n",
    "yourselves\n",
    "you've\n",
    "z\n",
    "zero\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jA3pwBH4rLIF"
   },
   "outputs": [],
   "source": [
    "# split the string by white space, get a stop word list\n",
    "stop_words = long_stop_word_list[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FFykQje69UkV"
   },
   "outputs": [],
   "source": [
    "# Remove from L all stopwords in “Long Stopword List”\n",
    "for x in stop_words:\n",
    "  if x in L:\n",
    "    L.remove(x)\n",
    "# Denote the cleaned set as W\n",
    "W = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/fengkaiqi/Library/Python/3.8/lib/python/site-packages (from nltk) (8.0.3)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.9.13-cp38-cp38-macosx_11_0_arm64.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.1/287.1 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, joblib, nltk\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/Users/fengkaiqi/Library/Python/3.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script nltk is installed in '/Users/fengkaiqi/Library/Python/3.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed joblib-1.2.0 nltk-3.7 regex-2022.9.13 tqdm-4.64.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ThdaVspbD7pn",
    "outputId": "dc3faae5-b5e2-43df-c202-e9d07e95fe6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fengkaiqi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus.reader.tagged import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "laJXHOaqFojk"
   },
   "outputs": [],
   "source": [
    "# before counting the number of times each word,\n",
    "# we should do tokenization, which could help us get a better word counting.\n",
    "words = list(W)\n",
    "tokens = []\n",
    "for x in words:\n",
    "  a = word_tokenize(x)\n",
    "  if len(a) >= 1:\n",
    "    for y in a:\n",
    "      tokens.append(y)\n",
    "  else:\n",
    "    tokens.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6ljCBOfRC4ao"
   },
   "outputs": [],
   "source": [
    "# Count the number of times each word in W appears among all reviews \n",
    "dictionary={}\n",
    "\n",
    "for x in tokens:\n",
    "    if x not in dictionary:\n",
    "        dictionary[x] = 1\n",
    "    else: dictionary[x] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7SFC9AMYJYQq"
   },
   "outputs": [],
   "source": [
    "# identify the top 500 words\n",
    "top500 = sorted(dictionary.items(),key = lambda x:x[1],reverse = True)[:500]\n",
    "top500_words = [x[0] for x in top500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "reviews = list(W)\n",
    "# create a vector with  zero\n",
    "vectors = np.zeros(shape=(len(reviews),500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize all reviews (“review/text” field) using these 500 words\n",
    "for i in range(len(reviews)):\n",
    "    for j in range(len(reviews[i])):\n",
    "        if reviews[i][j] in top500_words:\n",
    "            idx = top500_words.index(reviews[i][j])\n",
    "            vectors[i][idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.2-cp38-cp38-macosx_12_0_arm64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/fengkaiqi/Library/Python/3.8/lib/python/site-packages (from scikit-learn->sklearn) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/fengkaiqi/Library/Python/3.8/lib/python/site-packages (from scikit-learn->sklearn) (1.22.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/fengkaiqi/Library/Python/3.8/lib/python/site-packages (from scikit-learn->sklearn) (1.2.0)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=892b932f39c8cc87ab40512170163ac251a7da3dedc0254458a95ea35f95e7be\n",
      "  Stored in directory: /Users/fengkaiqi/Library/Caches/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-1.1.2 sklearn-0.0 threadpoolctl-3.1.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "oTATLI2KMH5U"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.decomposition import PCA \n",
    "import numpy as np\n",
    "\n",
    "X_pca = PCA(n_components=10).fit_transform(vectors) # decompose it into 10 dimensionalit, make k-means faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568454"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "opk3UOZzNlpp"
   },
   "outputs": [],
   "source": [
    "# set k = 10\n",
    "km = KMeans(n_clusters=10,random_state=0) \n",
    "km.fit(X_pca)\n",
    "\n",
    "centroid = km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "m-hu6wXmalqS"
   },
   "outputs": [],
   "source": [
    "# based on cos_sim to get the feature values\n",
    "def get_top5_words(vect_list,centroid,k,model_word):\n",
    "  features = []\n",
    "  ans = []\n",
    "  words = []\n",
    "  for i in range(len(vect_list)):\n",
    "    cos_sim = (vect_list[i].dot(centroid[k])) / (np.linalg.norm(vect_list[i]) * np.linalg.norm(centroid[k]))\n",
    "    features.append(cos_sim)\n",
    "\n",
    "  features = list(set(features))\n",
    "  features2 = sorted(features,reverse = True)[:5]\n",
    "\n",
    "  for x in features2:\n",
    "    ans.append(features.index(x))\n",
    "  \n",
    "  for x in ans:\n",
    "    idx = x % 500\n",
    "    words.append(model_word[idx])\n",
    "  return words,features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JoMv2XolPeSB",
    "outputId": "aa43a5d9-b5b0-4d17-f42f-aae4773b47cc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = []\n",
    "values = []\n",
    "for i in range(10):\n",
    "  a = get_top5_words(X_pca,centroid,i,top500_words)\n",
    "  words.append(a[0])\n",
    "  values.append(a[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SN_2o0Mkast5",
    "outputId": "41ffa88c-0e39-4d40-ea39-635d9e6ae518",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 1 has words:['so', 'why', 'since', 'anything', 'pepper']\n",
      "feature values: [0.9703305235274764, 0.9701312161416072, 0.9689683865200361, 0.968818246511888, 0.9684333651157725]\n",
      " \n",
      "cluster 2 has words:['quality', 'Also', 'week', 'quot', 'if']\n",
      "feature values: [0.9839298844737313, 0.9838444786983767, 0.983379099470887, 0.983308262561575, 0.9822610503744749]\n",
      " \n",
      "cluster 3 has words:['nuts', 'food', 'tea', 'jar', '--']\n",
      "feature values: [0.9972555612867994, 0.9971311335801858, 0.99699133547829, 0.9969871308993331, 0.9968668872255984]\n",
      " \n",
      "cluster 4 has words:['They', 'OK', 'since', 'others', '9']\n",
      "feature values: [0.9982101581176374, 0.997899332966335, 0.9978066189839725, 0.9977075252549791, 0.9976973067562002]\n",
      " \n",
      "cluster 5 has words:['fine', 'after', 'cost', \"n't\", 'more']\n",
      "feature values: [0.9927944007916638, 0.9926574749855307, 0.9926021393474771, 0.9925112082093684, 0.9924998925451557]\n",
      " \n",
      "cluster 6 has words:['brand', 'quickly', 'Great', 'both', '/']\n",
      "feature values: [0.9941500623573898, 0.9941284650943932, 0.994101896610087, 0.9940950473046523, 0.9940925003177176]\n",
      " \n",
      "cluster 7 has words:['better', 'drink', 'no', '[', 'as']\n",
      "feature values: [0.999892968950761, 0.9998927536462809, 0.999890984653224, 0.9998891520700361, 0.999887740072565]\n",
      " \n",
      "cluster 8 has words:['far', '#', 'as', 'about', '..']\n",
      "feature values: [0.9999996907235557, 0.9999996876858499, 0.9999989666901522, 0.9999955892225919, 0.9999913259692467]\n",
      " \n",
      "cluster 9 has words:['much', 'href=', 'Blue', 'said', 'package']\n",
      "feature values: [0.9711358786129025, 0.9703546385163551, 0.9703518047135811, 0.9702893246230967, 0.9702764400872174]\n",
      " \n",
      "cluster 10 has words:['see', 'things', 'enough', 'organic', 'long']\n",
      "feature values: [0.9999161428457454, 0.999895261908291, 0.9998921701738839, 0.9998607604302026, 0.9998427332305977]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# display top 5 words and value of each centroid\n",
    "for i in range(len(words)):\n",
    "  print('cluster ' + str(i+1) + ' has words:'  + str(words[i]))\n",
    "  print('feature values: '+ str(values[i]))\n",
    "  print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 78706),\n",
       " ('.', 78114),\n",
       " ('!', 68805),\n",
       " ('<', 56511),\n",
       " ('br', 51440),\n",
       " (')', 49907),\n",
       " (\"''\", 46796),\n",
       " ('>', 35087),\n",
       " ('...', 30722),\n",
       " ('(', 25194),\n",
       " ('``', 24649),\n",
       " ('/', 22529),\n",
       " (':', 18066),\n",
       " ('?', 15739),\n",
       " ('--', 14316),\n",
       " (\"'s\", 13158),\n",
       " ('....', 8110),\n",
       " ('http', 8079),\n",
       " ('$', 7873),\n",
       " (';', 7869),\n",
       " ('href=', 7794),\n",
       " ('*', 7654),\n",
       " (\"'\", 7583),\n",
       " ('..', 7152),\n",
       " ('I', 4357),\n",
       " ('/a', 3589),\n",
       " ('and', 3375),\n",
       " ('it', 3341),\n",
       " ('but', 2379),\n",
       " ('.....', 2368),\n",
       " ('&', 2366),\n",
       " ('a', 1989),\n",
       " ('%', 1971),\n",
       " (']', 1796),\n",
       " ('[', 1517),\n",
       " ('the', 1487),\n",
       " ('this', 1386),\n",
       " ('not', 1258),\n",
       " ('so', 1240),\n",
       " ('you', 1151),\n",
       " ('they', 1143),\n",
       " (\"n't\", 1126),\n",
       " ('that', 876),\n",
       " ('......', 848),\n",
       " ('just', 835),\n",
       " ('#', 778),\n",
       " ('good', 733),\n",
       " ('no', 701),\n",
       " ('great', 689),\n",
       " ('which', 687),\n",
       " ('-', 670),\n",
       " ('well', 657),\n",
       " ('them', 603),\n",
       " ('all', 589),\n",
       " ('taste', 585),\n",
       " ('or', 582),\n",
       " ('product', 576),\n",
       " ('coffee', 568),\n",
       " ('these', 562),\n",
       " ('flavor', 560),\n",
       " ('in', 555),\n",
       " ('for', 546),\n",
       " ('i', 544),\n",
       " ('is', 538),\n",
       " ('like', 510),\n",
       " ('The', 504),\n",
       " ('Amazon', 502),\n",
       " ('one', 485),\n",
       " ('if', 485),\n",
       " ('my', 479),\n",
       " ('there', 475),\n",
       " ('too', 471),\n",
       " ('do', 468),\n",
       " ('It', 461),\n",
       " ('tea', 452),\n",
       " ('me', 450),\n",
       " ('now', 450),\n",
       " ('again', 445),\n",
       " ('we', 443),\n",
       " ('more', 439),\n",
       " ('2', 433),\n",
       " ('as', 433),\n",
       " ('`', 424),\n",
       " ('s', 423),\n",
       " ('very', 422),\n",
       " ('.......', 415),\n",
       " (\"'m\", 410),\n",
       " ('to', 402),\n",
       " ('food', 397),\n",
       " ('time', 392),\n",
       " ('what', 383),\n",
       " ('with', 377),\n",
       " ('sugar', 374),\n",
       " ('1', 372),\n",
       " ('3', 368),\n",
       " ('then', 361),\n",
       " ('even', 360),\n",
       " ('etc', 357),\n",
       " ('delicious', 356),\n",
       " (\"'ll\", 355),\n",
       " ('price', 353),\n",
       " (\"'S\", 346),\n",
       " ('water', 340),\n",
       " ('4', 322),\n",
       " ('lt', 322),\n",
       " (\"'ve\", 316),\n",
       " ('6', 316),\n",
       " ('much', 307),\n",
       " ('chocolate', 307),\n",
       " ('can', 305),\n",
       " ('though', 303),\n",
       " ('at', 292),\n",
       " ('5', 292),\n",
       " ('she', 284),\n",
       " ('out', 284),\n",
       " ('box', 282),\n",
       " ('try', 282),\n",
       " (\"'d\", 280),\n",
       " ('bag', 279),\n",
       " ('@', 279),\n",
       " ('better', 278),\n",
       " ('day', 275),\n",
       " ('sweet', 273),\n",
       " ('maybe', 271),\n",
       " ('he', 271),\n",
       " ('milk', 269),\n",
       " (\"'re\", 265),\n",
       " ('here', 264),\n",
       " ('was', 261),\n",
       " ('up', 260),\n",
       " ('stuff', 257),\n",
       " ('on', 254),\n",
       " ('p', 254),\n",
       " ('especially', 252),\n",
       " ('This', 252),\n",
       " ('12', 249),\n",
       " ('best', 246),\n",
       " ('store', 243),\n",
       " ('bad', 241),\n",
       " ('love', 240),\n",
       " ('oil', 239),\n",
       " ('~', 239),\n",
       " ('........', 235),\n",
       " ('shipping', 234),\n",
       " ('of', 233),\n",
       " ('flavors', 230),\n",
       " ('however', 229),\n",
       " ('have', 229),\n",
       " ('perfect', 229),\n",
       " ('way', 228),\n",
       " ('right', 227),\n",
       " ('are', 224),\n",
       " ('sauce', 223),\n",
       " ('But', 223),\n",
       " ('when', 222),\n",
       " ('mix', 220),\n",
       " ('BUT', 220),\n",
       " ('They', 218),\n",
       " ('10', 218),\n",
       " ('only', 218),\n",
       " ('eacute', 216),\n",
       " ('did', 214),\n",
       " ('also', 212),\n",
       " ('had', 212),\n",
       " ('ingredients', 208),\n",
       " ('some', 207),\n",
       " ('thing', 203),\n",
       " ('will', 202),\n",
       " ('healthy', 201),\n",
       " ('lol', 201),\n",
       " ('salt', 201),\n",
       " ('something', 201),\n",
       " ('use', 199),\n",
       " ('So', 198),\n",
       " ('A', 198),\n",
       " ('its', 197),\n",
       " ('really', 196),\n",
       " ('cookies', 194),\n",
       " ('years', 191),\n",
       " ('tasty', 191),\n",
       " ('hot', 189),\n",
       " ('yes', 189),\n",
       " ('go', 189),\n",
       " ('fresh', 188),\n",
       " ('because', 188),\n",
       " ('be', 186),\n",
       " ('nothing', 185),\n",
       " ('about', 185),\n",
       " ('free', 185),\n",
       " ('from', 185),\n",
       " ('products', 185),\n",
       " ('treats', 183),\n",
       " ('why', 182),\n",
       " ('IT', 181),\n",
       " ('say', 181),\n",
       " ('yummy', 180),\n",
       " ('drink', 178),\n",
       " ('everything', 177),\n",
       " ('calories', 174),\n",
       " ('same', 174),\n",
       " ('would', 174),\n",
       " ('chicken', 173),\n",
       " ('yum', 172),\n",
       " ('brand', 172),\n",
       " ('organic', 171),\n",
       " ('snack', 171),\n",
       " ('cheese', 171),\n",
       " ('cereal', 170),\n",
       " ('does', 169),\n",
       " ('wonderful', 169),\n",
       " ('eat', 168),\n",
       " ('quot', 166),\n",
       " ('rice', 166),\n",
       " ('butter', 166),\n",
       " ('}', 165),\n",
       " ('8', 164),\n",
       " ('buy', 164),\n",
       " ('M', 163),\n",
       " ('dogs', 163),\n",
       " ('texture', 162),\n",
       " ('treat', 162),\n",
       " ('cup', 162),\n",
       " ('fat', 162),\n",
       " ('pack', 162),\n",
       " ('S', 162),\n",
       " ('bags', 161),\n",
       " ('chips', 161),\n",
       " ('days', 160),\n",
       " ('package', 160),\n",
       " ('amazing', 160),\n",
       " ('My', 159),\n",
       " ('yet', 159),\n",
       " ('favorite', 159),\n",
       " ('tastes', 158),\n",
       " ('If', 158),\n",
       " ('who', 157),\n",
       " ('.........', 157),\n",
       " ('excellent', 156),\n",
       " ('plus', 155),\n",
       " ('know', 155),\n",
       " ('You', 155),\n",
       " ('off', 155),\n",
       " ('ever', 154),\n",
       " ('before', 153),\n",
       " ('after', 153),\n",
       " ('nice', 152),\n",
       " ('until', 152),\n",
       " ('easy', 151),\n",
       " ('20', 151),\n",
       " ('bar', 150),\n",
       " ('cream', 150),\n",
       " ('back', 150),\n",
       " ('stores', 150),\n",
       " ('size', 150),\n",
       " ('And', 149),\n",
       " ('anything', 149),\n",
       " ('strong', 148),\n",
       " ('away', 148),\n",
       " ('either', 148),\n",
       " ('thanks', 148),\n",
       " ('B', 148),\n",
       " ('expensive', 148),\n",
       " ('candy', 148),\n",
       " ('natural', 147),\n",
       " ('order', 145),\n",
       " ('money', 145),\n",
       " ('dog', 145),\n",
       " ('Great', 145),\n",
       " ('less', 144),\n",
       " ('both', 144),\n",
       " ('work', 143),\n",
       " ('oh', 143),\n",
       " ('first', 143),\n",
       " ('enough', 143),\n",
       " ('beans', 143),\n",
       " ('syrup', 142),\n",
       " ('juice', 141),\n",
       " ('diet', 141),\n",
       " ('down', 140),\n",
       " ('soup', 139),\n",
       " ('review', 139),\n",
       " ('Starbucks', 138),\n",
       " ('problem', 138),\n",
       " ('Well', 137),\n",
       " ('WOW', 137),\n",
       " ('bottle', 137),\n",
       " ('foods', 136),\n",
       " ('powder', 135),\n",
       " ('Tea', 135),\n",
       " ('Coffee', 135),\n",
       " ('life', 134),\n",
       " ('ca', 134),\n",
       " ('deal', 134),\n",
       " ('still', 133),\n",
       " ('make', 133),\n",
       " ('home', 133),\n",
       " ('bars', 133),\n",
       " ('bread', 132),\n",
       " ('quality', 132),\n",
       " ('..........', 131),\n",
       " ('reviews', 129),\n",
       " ('fruit', 129),\n",
       " ('kind', 129),\n",
       " ('item', 128),\n",
       " ('100', 128),\n",
       " ('packaging', 127),\n",
       " ('ok', 127),\n",
       " ('7', 127),\n",
       " ('kids', 127),\n",
       " ('months', 127),\n",
       " ('oz', 127),\n",
       " ('things', 127),\n",
       " ('morning', 126),\n",
       " ('Organic', 125),\n",
       " ('bitter', 125),\n",
       " ('please', 125),\n",
       " ('mouth', 125),\n",
       " ('amazon', 125),\n",
       " ('made', 125),\n",
       " ('15', 125),\n",
       " ('old', 125),\n",
       " ('almost', 125),\n",
       " ('enjoy', 125),\n",
       " ('YUM', 124),\n",
       " ('once', 124),\n",
       " ('30', 124),\n",
       " ('C', 124),\n",
       " ('tried', 124),\n",
       " ('fast', 123),\n",
       " ('D', 123),\n",
       " ('meal', 123),\n",
       " ('long', 123),\n",
       " ('pasta', 123),\n",
       " ('get', 123),\n",
       " ('two', 123),\n",
       " ('could', 122),\n",
       " ('although', 122),\n",
       " ('anyway', 121),\n",
       " ('since', 121),\n",
       " ('NOT', 121),\n",
       " ('AND', 121),\n",
       " ('{', 121),\n",
       " ('LOL', 120),\n",
       " ('serving', 120),\n",
       " ('myself', 120),\n",
       " ('else', 120),\n",
       " ('OK', 119),\n",
       " ('stars', 119),\n",
       " ('minutes', 118),\n",
       " ('blend', 118),\n",
       " ('smooth', 118),\n",
       " ('nuts', 118),\n",
       " ('24', 118),\n",
       " ('Thanks', 118),\n",
       " ('said', 117),\n",
       " ('each', 117),\n",
       " ('year', 117),\n",
       " ('small', 117),\n",
       " ('far', 117),\n",
       " ('house', 117),\n",
       " ('meat', 116),\n",
       " ('9', 116),\n",
       " ('coconut', 116),\n",
       " ('tasting', 115),\n",
       " ('brands', 115),\n",
       " ('smell', 115),\n",
       " ('ones', 115),\n",
       " ('Good', 115),\n",
       " ('tasted', 114),\n",
       " ('her', 114),\n",
       " ('honey', 114),\n",
       " ('50', 114),\n",
       " ('teas', 114),\n",
       " ('think', 113),\n",
       " ('market', 113),\n",
       " ('awesome', 113),\n",
       " ('case', 113),\n",
       " ('protein', 112),\n",
       " ('recipe', 112),\n",
       " ('different', 112),\n",
       " ('wrong', 112),\n",
       " ('crunchy', 112),\n",
       " ('25', 112),\n",
       " ('cookie', 112),\n",
       " ('wow', 111),\n",
       " ('crackers', 111),\n",
       " ('find', 111),\n",
       " ('No', 110),\n",
       " ('fine', 110),\n",
       " ('boxes', 109),\n",
       " ('most', 109),\n",
       " ('flour', 108),\n",
       " ('over', 108),\n",
       " ('spicy', 108),\n",
       " ('service', 108),\n",
       " ('gum', 108),\n",
       " ('/span', 108),\n",
       " ('has', 107),\n",
       " ('popcorn', 107),\n",
       " ('However', 107),\n",
       " ('other', 106),\n",
       " ('ASIN', 106),\n",
       " ('company', 106),\n",
       " ('light', 106),\n",
       " ('okay', 106),\n",
       " ('sometimes', 106),\n",
       " ('Now', 106),\n",
       " ('add', 106),\n",
       " ('Wow', 106),\n",
       " ('cake', 105),\n",
       " ('dry', 105),\n",
       " ('used', 104),\n",
       " ('high', 104),\n",
       " ('except', 104),\n",
       " ('Oh', 104),\n",
       " ('ago', 104),\n",
       " ('hard', 104),\n",
       " ('while', 104),\n",
       " ('40', 103),\n",
       " ('got', 103),\n",
       " ('gift', 103),\n",
       " ('disappointed', 102),\n",
       " ('week', 102),\n",
       " ('t', 102),\n",
       " ('how', 102),\n",
       " ('Bob', 102),\n",
       " ('without', 102),\n",
       " ('happy', 102),\n",
       " ('were', 102),\n",
       " ('cinnamon', 102),\n",
       " ('top', 101),\n",
       " ('people', 101),\n",
       " ('around', 101),\n",
       " ('...........', 101),\n",
       " ('today', 101),\n",
       " ('Not', 101),\n",
       " ('Green', 101),\n",
       " ('added', 100),\n",
       " ('soft', 100),\n",
       " ('formula', 100),\n",
       " ('by', 100),\n",
       " ('fantastic', 100),\n",
       " ('anymore', 100),\n",
       " ('works', 100),\n",
       " ('available', 99),\n",
       " ('cans', 99),\n",
       " ('cats', 99),\n",
       " ('seeds', 99),\n",
       " ('Yes', 98),\n",
       " ('corn', 98),\n",
       " ('fiber', 98),\n",
       " ('jar', 98),\n",
       " ('later', 98),\n",
       " ('everyone', 98),\n",
       " ('noodles', 97),\n",
       " ('salty', 97),\n",
       " ('low', 97),\n",
       " ('Also', 97),\n",
       " ('thought', 97),\n",
       " ('see', 96),\n",
       " ('us', 96),\n",
       " ('We', 96),\n",
       " ('oatmeal', 96),\n",
       " ('chewy', 96),\n",
       " ('others', 96),\n",
       " ('cooking', 96),\n",
       " ('instead', 96),\n",
       " ('half', 96),\n",
       " ('world', 96),\n",
       " ('eggs', 95),\n",
       " ('cost', 95),\n",
       " ('rich', 95),\n",
       " ('month', 95),\n",
       " ('Just', 94),\n",
       " ('green', 94),\n",
       " ('THIS', 94),\n",
       " ('Enjoy', 94),\n",
       " ('breakfast', 94),\n",
       " ('cheaper', 94),\n",
       " ('makes', 93),\n",
       " ('quickly', 93),\n",
       " ('online', 92),\n",
       " ('bite', 92),\n",
       " ('thank', 92),\n",
       " ('vanilla', 92),\n",
       " ('times', 92),\n",
       " ('weight', 92),\n",
       " ('another', 92),\n",
       " ('color', 91),\n",
       " ('yuck', 91),\n",
       " ('never', 91),\n",
       " ('family', 91),\n",
       " ('Love', 91),\n",
       " ('What', 91),\n",
       " ('purchase', 91),\n",
       " ('Blue', 91),\n",
       " ('UPDATE', 91),\n",
       " ('pepper', 90),\n",
       " ('pieces', 90),\n",
       " ('sorry', 90)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display top 500 words and its count.\n",
    "top500"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
